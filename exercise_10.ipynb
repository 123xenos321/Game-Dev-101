{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "exercise_10.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/123xenos321/Game-Dev-101/blob/master/exercise_10.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AZ2XOzT2o7Ov"
      },
      "source": [
        "# Gradient Descent: Boston Housing\n",
        "\n",
        "In this exercise, let's use Gradient Descent based methods to predict boston house prices. The 14 variables are:\n",
        "\n",
        "* CRIM per capita crime rate by town\n",
        "\n",
        "* ZN proportion of residential land zoned for lots over 25,000 sq.ft.\n",
        "\n",
        "* INDUS proportion of non-retail business acres per town\n",
        "\n",
        "* CHAS Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
        "\n",
        "* NOX nitric oxides concentration (parts per 10 million)\n",
        "\n",
        "* RM average number of rooms per dwelling\n",
        "\n",
        "* AGE proportion of owner-occupied units built prior to 1940\n",
        "\n",
        "* DIS weighted distances to five Boston employment centres\n",
        "\n",
        "* RAD index of accessibility to radial highways\n",
        "\n",
        "* TAX full-value property-tax rate per $10,000\n",
        "\n",
        "* PTRATIO pupil-teacher ratio by town\n",
        "\n",
        "* B 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
        "\n",
        "* LSTAT % lower status of the population\n",
        "\n",
        "* MEDV Median value of owner-occupied homes in $1000’s\n",
        "\n",
        "\n",
        "The target variable is **Median value of owner-occupied homes in $1000’s**.\n",
        "\n",
        "Split the dataset into train/test. Build a regression model using **SGDRegressor** from sklearn. Compute the mean square error on the test set. Describle how the loss changes as we have more epochs (Hint: set a larger verbose parameter).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lut6sGrsdtdG"
      },
      "source": [
        "## (1) Split the dataset into train/test. Build a regression model using **SGDRegressor** from sklearn. Compute the mean square error on the test set. Describle how the loss changes as we have more epochs (Hint: set a larger verbose parameter).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O8ufSHkcGVFC",
        "outputId": "3498c398-ef5e-4b27-eddb-b61d9450ae8c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import preprocessing\n",
        "\n",
        "data = load_boston()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "target_name = 'MEDV'\n",
        "# Hint: Standardize features by removing the mean and scaling to unit variance\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.1,random_state=99)\n",
        "# insert your code here\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "X_train= scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "print('Number of train = {}'.format(len(y_train)))\n",
        "print('Number of test = {}'.format(len(y_test)))\n",
        "reg = SGDRegressor(max_iter=50,tol=1e-3,verbose=10)\n",
        "reg.fit(X_train,y_train)\n",
        "y_train_prediction = reg.predict(X_train)\n",
        "y_test_prediction = reg.predict(X_test)\n",
        "print('Train MSE = {}'.format(mean_squared_error(y_train,y_train_prediction)))\n",
        "print('Test MSE ={}'.format(mean_squared_error(y_test,y_test_prediction)))\n",
        "print(reg.coef_)"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of train = 455\n",
            "Number of test = 51\n",
            "-- Epoch 1\n",
            "Norm: 4.86, NNZs: 13, Bias: 16.637025, T: 455, Avg. loss: 93.280399\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 2\n",
            "Norm: 5.31, NNZs: 13, Bias: 20.191613, T: 910, Avg. loss: 21.477900\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 3\n",
            "Norm: 5.63, NNZs: 13, Bias: 21.470702, T: 1365, Avg. loss: 14.063938\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 4\n",
            "Norm: 5.49, NNZs: 13, Bias: 21.996813, T: 1820, Avg. loss: 12.670777\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 5\n",
            "Norm: 5.71, NNZs: 13, Bias: 22.267478, T: 2275, Avg. loss: 12.359664\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 6\n",
            "Norm: 5.84, NNZs: 13, Bias: 22.429453, T: 2730, Avg. loss: 12.108272\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 7\n",
            "Norm: 5.96, NNZs: 13, Bias: 22.461638, T: 3185, Avg. loss: 11.977729\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 8\n",
            "Norm: 5.99, NNZs: 13, Bias: 22.514320, T: 3640, Avg. loss: 11.957772\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 9\n",
            "Norm: 6.02, NNZs: 13, Bias: 22.558825, T: 4095, Avg. loss: 11.837068\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 10\n",
            "Norm: 6.17, NNZs: 13, Bias: 22.577878, T: 4550, Avg. loss: 11.815951\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 11\n",
            "Norm: 6.21, NNZs: 13, Bias: 22.573876, T: 5005, Avg. loss: 11.779920\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 12\n",
            "Norm: 6.24, NNZs: 13, Bias: 22.566880, T: 5460, Avg. loss: 11.729683\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 13\n",
            "Norm: 6.29, NNZs: 13, Bias: 22.564486, T: 5915, Avg. loss: 11.679171\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 14\n",
            "Norm: 6.32, NNZs: 13, Bias: 22.579340, T: 6370, Avg. loss: 11.682855\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 15\n",
            "Norm: 6.41, NNZs: 13, Bias: 22.589393, T: 6825, Avg. loss: 11.641036\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 16\n",
            "Norm: 6.44, NNZs: 13, Bias: 22.566321, T: 7280, Avg. loss: 11.648637\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 17\n",
            "Norm: 6.53, NNZs: 13, Bias: 22.580942, T: 7735, Avg. loss: 11.614933\n",
            "Total training time: 0.00 seconds.\n",
            "-- Epoch 18\n",
            "Norm: 6.55, NNZs: 13, Bias: 22.587590, T: 8190, Avg. loss: 11.606802\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 19\n",
            "Norm: 6.59, NNZs: 13, Bias: 22.581222, T: 8645, Avg. loss: 11.590117\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 20\n",
            "Norm: 6.64, NNZs: 13, Bias: 22.600487, T: 9100, Avg. loss: 11.578961\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 21\n",
            "Norm: 6.57, NNZs: 13, Bias: 22.610023, T: 9555, Avg. loss: 11.517782\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 22\n",
            "Norm: 6.70, NNZs: 13, Bias: 22.603322, T: 10010, Avg. loss: 11.554521\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 23\n",
            "Norm: 6.65, NNZs: 13, Bias: 22.605840, T: 10465, Avg. loss: 11.549824\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 24\n",
            "Norm: 6.73, NNZs: 13, Bias: 22.606174, T: 10920, Avg. loss: 11.556893\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 25\n",
            "Norm: 6.73, NNZs: 13, Bias: 22.614636, T: 11375, Avg. loss: 11.538720\n",
            "Total training time: 0.01 seconds.\n",
            "-- Epoch 26\n",
            "Norm: 6.76, NNZs: 13, Bias: 22.605354, T: 11830, Avg. loss: 11.541972\n",
            "Total training time: 0.01 seconds.\n",
            "Convergence after 26 epochs took 0.01 seconds\n",
            "Train MSE = 22.674472176600982\n",
            "Test MSE =16.81408208974772\n",
            "[-0.84699679  0.9842938  -0.08422454  0.77432627 -1.66767268  2.80283092\n",
            " -0.11751809 -3.03704561  1.68735425 -1.17871352 -2.00218157  0.75117299\n",
            " -3.83274682]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GoIfDRjrdsRa"
      },
      "source": [
        "# Logistic Regression and Tree-based Methods: Breast Cancer detection\n",
        "\n",
        "In this exercise, let's use Gradient Descent based methods to classify malignent/benign breast tumor. The 30 variables are mean/error/worst of the following 10 metrics:\n",
        "\n",
        "* radius\n",
        "\n",
        "* texture\n",
        "\n",
        "* perimeter\n",
        "\n",
        "* area\n",
        "\n",
        "* smoothness\n",
        "\n",
        "* compactness\n",
        "\n",
        "* concavity\n",
        "\n",
        "* concave points\n",
        "\n",
        "* symmetry\n",
        "\n",
        "* fractal dimension\n",
        "\n",
        "\n",
        "The goal is to predict whether or not the tumor is malignent or benign.\n",
        "\n",
        "(1) Play with the dataset. Show the number of positives and negatives. The split the dataset into train and test, where test is 10% of the data.\n",
        "\n",
        "(2) Using logistic regression to train on the train set. Then test model accuracy, F1 score for negatives, F1 score for positives, F1 macro, and F1 micro.\n",
        "\n",
        "(3) Build a random forest model to train on the train set. Then test the same metrics as in step (2).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y-OI1-dQ9Tgy"
      },
      "source": [
        "## (1) Play with the dataset. Show the number of positives and negatives. The split the dataset into train and test, where test is 10% of the data.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6-UOC0QcSjXk",
        "outputId": "7bc65486-ed16-499d-c530-6e5c6898f4c6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import SGDRegressor, LinearRegression\n",
        "import numpy as np\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn import preprocessing\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "feature_names = data.feature_names\n",
        "target_names=data.target_names\n",
        "print('Number of positives = {}'.format(np.sum(y==1)))\n",
        "print('Numebr of negatives = {}'.format(np.sum(y==0)))\n",
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.1,random_state =99)\n",
        "scaler = preprocessing.StandardScaler().fit(X_train)\n",
        "X_train=scaler.transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "for i in range(X.shape[1]):\n",
        "    print('Variable: Mean of {} = {}'.format(feature_names[i],np.mean(X[:,i])))\n",
        "\n",
        "print('Number of train = {}'.format(len(y_train)))\n",
        "print('Number of test = {}'.format(len(y_test)))\n",
        "\n",
        "# insert your code here\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of positives = 357\n",
            "Numebr of negatives = 212\n",
            "Variable: Mean of mean radius = 14.127291739894552\n",
            "Variable: Mean of mean texture = 19.289648506151142\n",
            "Variable: Mean of mean perimeter = 91.96903339191564\n",
            "Variable: Mean of mean area = 654.8891036906855\n",
            "Variable: Mean of mean smoothness = 0.0963602811950791\n",
            "Variable: Mean of mean compactness = 0.10434098418277679\n",
            "Variable: Mean of mean concavity = 0.0887993158172232\n",
            "Variable: Mean of mean concave points = 0.04891914586994728\n",
            "Variable: Mean of mean symmetry = 0.18116186291739894\n",
            "Variable: Mean of mean fractal dimension = 0.06279760984182776\n",
            "Variable: Mean of radius error = 0.40517205623901575\n",
            "Variable: Mean of texture error = 1.2168534270650264\n",
            "Variable: Mean of perimeter error = 2.8660592267135327\n",
            "Variable: Mean of area error = 40.337079086116\n",
            "Variable: Mean of smoothness error = 0.007040978910369069\n",
            "Variable: Mean of compactness error = 0.025478138840070295\n",
            "Variable: Mean of concavity error = 0.03189371634446397\n",
            "Variable: Mean of concave points error = 0.011796137082601054\n",
            "Variable: Mean of symmetry error = 0.02054229876977153\n",
            "Variable: Mean of fractal dimension error = 0.0037949038664323374\n",
            "Variable: Mean of worst radius = 16.269189806678387\n",
            "Variable: Mean of worst texture = 25.677223198594024\n",
            "Variable: Mean of worst perimeter = 107.26121265377857\n",
            "Variable: Mean of worst area = 880.5831282952548\n",
            "Variable: Mean of worst smoothness = 0.13236859402460457\n",
            "Variable: Mean of worst compactness = 0.25426504393673116\n",
            "Variable: Mean of worst concavity = 0.27218848330404216\n",
            "Variable: Mean of worst concave points = 0.11460622319859401\n",
            "Variable: Mean of worst symmetry = 0.2900755711775044\n",
            "Variable: Mean of worst fractal dimension = 0.0839458172231986\n",
            "Number of train = 512\n",
            "Number of test = 57\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZqRIz6OIiD2C"
      },
      "source": [
        "## (2) Using logistic regression to train on the train set. Then test model accuracy, F1 score for negatives, F1 score for positives.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5YImQT4xeDmz"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, confusion_matrix, precision_recall_fscore_support\n",
        "\n",
        "# insert your code here"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNqyZ-e58RST"
      },
      "source": [
        "## (3) Build a random forest model to train on the train set. Then test the same metrics as in step (2).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zu4Kzh9O8TLW"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# insert your code here"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}